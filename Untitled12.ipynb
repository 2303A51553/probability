{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWtr9t9e0y/KdQABwGzxHr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51553/probability/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIXGojk8NVYN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "# Sample dataset\n",
        "data = {\n",
        "'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "'Feature2': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
        "'Feature3': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
        "'Target': [5, 7, 9, 11, 13, 15, 17, 19, 21, 23]\n",
        "}\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "# Define independent (X) and dependent (y) variables\n",
        "X = df[['Feature1', 'Feature2', 'Feature3']]\n",
        "y = df['Target']\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Initialize and train the regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "# Print model coefficients and intercept\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "2 | P a g e E R R O J U R A M A K R I S H N A\n",
        "APPLICATIONS OF DATA MINING\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared Score: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Stepwise Explanation of Simple Linear Regression\n",
        "Linear regression is a statistical method used to model the relationship between an\n",
        "independent variable (X) and a dependent variable (Y) by fitting a linear equation to observed\n",
        "data.\n",
        "Step 1: Understanding the Equation\n",
        "The equation of a simple linear regression model is:\n",
        "Y=mX+b+ε\n",
        "Where:\n",
        "• Y = Dependent variable (Target)\n",
        "• X = Independent variable (Feature)\n",
        "• m= Slope of the line (Coefficient)\n",
        "• b = Intercept (where the line crosses the Y-axis)\n",
        "• ε = Error term (Random noise in data)\n",
        "Step 2: Generating Data\n",
        "We generate synthetic data to fit a linear regression model.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Randomly generate X values\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "# Define the true function and add some noise\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "# Plot the generated data\n",
        "plt.scatter(X, y, color=\"blue\")\n",
        "plt.xlabel(\"Independent Variable (X)\")\n",
        "plt.ylabel(\"Dependent Variable (Y)\")\n",
        "plt.title(\"Generated Data for Linear Regression\")\n",
        "plt.show()\n",
        "Key Takeaways:\n",
        "• X values are randomly generated between 0 and 2.\n",
        "• Y values follow a linear pattern with some added noise.\n",
        "Step 3: Splitting Data into Training and Testing Sets\n",
        "We divide the data into training (to train the model) and testing (to evaluate the model).\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "Why?\n",
        "• Training data is used to build the model.\n",
        "• Testing data is used to check the model’s performance on unseen data.\n",
        "Step 4: Training the Linear Regression Model\n",
        "We use Scikit-learn's LinearRegression to train our model.\n",
        "2 | P a g e E R R O J U R A M A K R I S H N A\n",
        "APPLICATIONS OF DATA MINING\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# Get the learned parameters\n",
        "m = model.coef_ # Slope\n",
        "b = model.intercept_ # Intercept\n",
        "print(f\"Slope (m): {m}\")\n",
        "print(f\"Intercept (b): {b}\")\n",
        "What happens here?\n",
        "• The model learns the best-fitting line by finding mmm and bbb that minimize the\n",
        "error.\n",
        "• These values are found using Ordinary Least Squares (OLS) method.\n",
        "Step 5: Making Predictions\n",
        "Now, we use the trained model to predict YYY values for the test set.\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "Why do we predict?\n",
        "• To evaluate how well our model generalizes to new data.\n",
        "Step 6: Visualizing the Regression Line\n",
        "We plot the original data and overlay the regression line.\n",
        "plt.scatter(X, y, color=\"blue\", label=\"Actual Data\")\n",
        "plt.plot(X, model.predict(X), color=\"red\", linewidth=2, label=\"Regression Line\")\n",
        "plt.xlabel(\"Independent Variable (X)\")\n",
        "plt.ylabel(\"Dependent Variable (Y)\")\n",
        "plt.legend()\n",
        "plt.title(\"Linear Regression Fit\")\n",
        "plt.show()\n",
        "What to observe?\n",
        "• The red line represents the best-fitting regression line.\n",
        "• The scatter points show the actual data.\n",
        "Step 7: Evaluating the Model\n",
        "To check the accuracy of our model, we calculate the Mean Squared Error (MSE).\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "Interpretation:\n",
        "• Low MSE → The model predictions are close to actual values.\n",
        "• High MSE → The model is not fitting well.\n",
        "Step 8: Interpreting the Model\n",
        "3 | P a g e E R R O J U R A M A K R I S H N A\n",
        "APPLICATIONS OF DATA MINING\n",
        "• If mmm (slope) is positive, there is a positive correlation between X and Y (as X\n",
        "increases, Y increases).\n",
        "• If mmm (slope) is negative, there is a negative correlation (as X increases, Y\n",
        "decreases).\n",
        "• The closer the data points are to the regression line, the better the model.\n",
        "Final Summary\n",
        "Step Action Key Takeaway\n",
        "1 Understand equation Y=mX+b Defines the relationship between X and Y\n",
        "2 Generate data Create a dataset with noise\n",
        "3 Split data Separate training and testing sets\n",
        "4 Train model Learn the best-fit line"
      ],
      "metadata": {
        "id": "KBgRb_FxOE1u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}